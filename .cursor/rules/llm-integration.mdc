---
globs: agents/**/*.py
---

# LLM Integration and Error Handling

This project uses Mistral AI via LangChain. Follow these patterns for consistent LLM integration.

## LLM Configuration

### Environment Setup
```python
import os
MODEL_NAME = os.getenv("MISTRAL_MODEL_NAME", "mistral-small-latest")

# Initialize LLM
from langchain_mistralai import ChatMistralAI
llm = ChatMistralAI(
    model=MODEL_NAME,
    temperature=0.5,  # 0.5 for structured, 0.7 for creative
)
```

### Required Environment Variables
- `MISTRAL_API_KEY` - Required for API access
- `MISTRAL_MODEL_NAME` - Optional, defaults to "mistral-small-latest"

## Output Parsing Patterns

### Structured Output (Pydantic)
```python
from langchain.output_parsers import RetryWithErrorOutputParser, PydanticOutputParser

# Create parser
parser = PydanticOutputParser(pydantic_object=YourModel)

# Create retry parser with configurable retries
fix_parser = RetryWithErrorOutputParser.from_llm(
    llm=llm,
    parser=parser,
    max_retries=max_retries,  # Make this configurable
)
```

### Error Handling Strategy
```python
try:
    # First attempt - direct parsing
    return parser.parse(raw_response)
except Exception:
    # Second attempt - retry with error feedback
    fixed = fix_parser.parse_with_prompt(
        completion=raw_response,
        prompt_value=retry_prompt.format_prompt(
            schema=parser.get_format_instructions(),
            # Add context for retry
        ),
    )
    return fixed
```

## Async LLM Calls
```python
# For parallel processing
async def generate_content(...):
    response = await llm.ainvoke(prompt)
    return response.content.strip()
```

## Prompt Engineering Best Practices

### Structure
1. **Context**: Provide clear role and context
2. **Instructions**: Step-by-step requirements  
3. **Constraints**: Exact format and validation rules
4. **Examples**: When helpful for complex outputs
5. **Language**: Explicit language requirements

### Format Instructions
- Always include `{format_instructions}` for Pydantic outputs
- Use clear JSON formatting requirements
- Specify exact field requirements
- Include language constraints explicitly

### Multi-language Support
```python
prompt = """
CRITICAL LANGUAGE INSTRUCTION:
Write the ENTIRE content exclusively in {language}. 
Every single word, phrase, sentence, and paragraph must be in {language}.
Do not mix languages or include any English text unless specifically requested.
"""
```

## Error Recovery Patterns

### Fallback Content
```python
try:
    # Primary generation
    content = await generate_primary_content(...)
except Exception as e:
    print(f"✗ Error: {e}")
    # Provide meaningful fallback
    content = f"Content for {title} (generation failed, please regenerate)"
```

### Retry Logic
- Make `max_retries` configurable (typically 3-5)
- Log retry attempts for debugging
- Use exponential backoff for rate limiting
- Fail gracefully with informative messages

### Progress Feedback
```python
print(f"✓ Generated content for {context}")  # Success
print(f"✗ Error generating content: {error}")  # Failure
```

## Model Selection Guidelines
- `mistral-small-latest`: Fast, cost-effective for structured output
- `mistral-medium-latest`: Better for complex reasoning
- `mistral-large-latest`: Highest quality for critical content

Temperature settings:
- 0.0-0.3: Deterministic, structured output
- 0.5: Balanced creativity and consistency  
- 0.7-1.0: Creative content generation